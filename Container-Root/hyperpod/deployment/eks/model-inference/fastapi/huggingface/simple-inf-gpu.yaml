kind: Deployment
apiVersion: apps/v1
metadata:
  name: simple-inf-gpu
  labels:
    app: simple-inf-gpu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: simple-inf-gpu
  template:
    metadata:
      labels:
        app: simple-inf-gpu
    spec:
      #nodeSelector:
      #  node.kubernetes.io/instance-type: "ml.g5.8xlarge"
      containers:
      - name: main
        image: pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel
        imagePullPolicy: Always
        env:
        - name: processor
          value: gpu
        command:
        - /bin/sh
        - -c
        - |
          #!/bin/bash          
          apt update
          apt install -y curl vim jq
          pip install transformers configparser fastapi uvicorn
          curl -L -o config.properties https://raw.githubusercontent.com/aws-samples/aws-do-hyperpod/refs/heads/main/Container-Root/hyperpod/deployment/eks/model-inference/fastapi/huggingface/simple-inf/Container-Root/config.properties
          sed -i -e "s/processor=cpu/processor=gpu/g" ./config.properties
          . ./config.properties
          mkdir -p $model_save_path
          export HF_HOME=$model_save_path
          curl -L -o fastapi-server.py https://raw.githubusercontent.com/aws-samples/aws-do-hyperpod/refs/heads/main/Container-Root/hyperpod/deployment/eks/model-inference/fastapi/huggingface/simple-inf/Container-Root/fastapi-server.py
          uvicorn fastapi-server:app --host 0.0.0.0 --port 8080
        ports:
        - name: pod-port
          containerPort: 8080
        resources:
          limits:
          #  cpu: 1
          #  memory: 4Gi
            nvidia.com/gpu: 1
          requests:
          #  cpu: 1
          #  memory: 1Gi
            nvidia.com/gpu: 1
---
kind: Service
apiVersion: v1
metadata:
  name: simple-inf-gpu
  labels:
    app: simple-inf-gpu
spec:
  ports:
  - name: svc-port
    port: 8080
    targetPort: pod-port
  type: ClusterIP
  selector:
    app: simple-inf-gpu
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: curl-inf-gpu
  labels:
    app: curl-inf-gpu
spec:
  replicas: 5
  selector:
    matchLabels:
      app: curl-inf-gpu
  template:
    metadata:
      labels:
        app: curl-inf-gpu
    spec:
      #nodeSelector:
      #  beta.kubernetes.io/instance-type: "ml.m5.2xlarge"
      containers:
      - name: curl
        image: iankoulski/do-curl:latest
        imagePullPolicy: Always
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          while true; do 
              echo ""
              curl http://simple-inf-gpu:8080
              echo ""
              curl http://simple-inf-gpu:8080/predictions/model0
              sleep 0.1
          done

